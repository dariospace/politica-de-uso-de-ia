# Política de uso de inteligencia artificial en la asignatura
## Elaborada por Juan David Gutiérrez, Universidad del Rosario
Versión 4.0 (30/01/2023). Licencia C.C. BY 4.0.

1. Justificación de la política:
   * Usamos herramientas de inteligencia artificial diariamente. De manera consciente o inconsciente, prácticamente todos usamos algún tipo de inteligencia artificial (IA) en nuestra vida diaria, incluso para realizar actividades en la universidad. Por ejemplo: cuando estamos escribiendo un mensaje de texto o un correo y la herramienta de “autocompletar” nos sugiere cómo terminar de escribir la oración; cuando buscamos información a través de motores de búsqueda en Internet; cuando usamos software de procesamiento de texto que sugiere correcciones gramaticales; y, cuando usamos herramientas de automatizadas de traducción, entre otros. Además, ahora existen plataformas web y móviles que permiten acceder a herramientas de IA para generar textos y contenidos audiovisuales. Existen usos adecuados de dichas tecnologías que no solo pueden resultar útiles en la universidad sino también en el plano profesional. Por ejemplo, diferentes herramientas de IA hoy en día contribuyen en las actividadesde todas las etapas de los procesos de política pública.
   * Pero ciertos usos de la IA son riesgosos, por tanto, su uso debe ser informado, ético y
responsable. Las herramientas de IA no son idóneas para realizar cualquier tipo de
actividad y determinados tipos de uso pueden ser contraproducentes para el proceso
pedagógico. Algunos usos de la IA pueden generar riesgostanto para los usuarios como
para terceros. Además, como se explica en la sección 4 de esta política, es
indispensable ser conscientes de las implicaciones éticas, ambientales y de derechos
humanos asociadas al uso de estas herramientas.
2. Objetivos de la política:
   * Prevención. Prevenir situaciones en las cuales los estudiantes consciente o
inconscientemente incurran en deshonestidad académica.
   * Promoción de uso responsable. Promover el uso responsable y ético de tecnologías
emergentes, como la IA, en los procesos de aprendizaje y para su futuro uso en la vida
profesional.
3. Reglas de uso de IA dentro y fuera del salón de clase:
   1. Regla general. En esta asignatura está permitido el uso de IA como herramienta de
soporte para realización de las diferentes actividades de aprendizaje. Los parámetros
para el uso de estas herramientas que se describen a continuación distinguen entre
herramientas de “bajo riesgo” y de “alto riesgo”, según los riesgos que la respectiva IA
genere para el proceso pedagógico, para los usuarios y para terceros.
   2. Uso de IA de riesgo bajo. Podrán usar libremente herramientas de IA de bajo riesgo, es
decir aquellas que les permitan corregir o revisar contenidos generados por los propios
estudiantes o aquellas que les permitan recolectar y procesar datos. Por ejemplo, las
herramientas de corrección gramatical, de traducción, de transcripción audio-texto, y
de búsqueda de información en Internet, entre otras.
   3. Uso de IA de riesgo alto. Son de alto riesgo, las herramientas de IA que generen
contenidos (texto y/o audiovisual), es decir, IA generadora de texto (Generative AI,
como ChatGPT) e IA de difusión estable (Stable Diffusion, como DALL-E). Solo podrán
incluir en sus entregas individuales y colectivas contenidos generados por IA cuando
cumplan con dos requisitos: 1. Que indiquen detallada y expresamente qué
herramienta usaron y cómo la usaron (un requisito que actualmente solicitan revistas
científicas). 2. Que sea posible distinguir qué fue escrito o producido directamente por
ustedes y qué fue generado por una herramienta de IA. Respecto de este punto, aplican
las reglas de citación generales, por ejemplo, usar comillas si incluyen apartes textuales.
Las infracciones a esta política, particularmente respecto del segundo requisito, serán
abordadas bajo un enfoque similar al que aplica en casos de plagio. Además, ChatGPT
no es una persona, luego no puede ser considerado su coautor (así como ustedes
tampoco incluyen a Google como su coautor en sus trabajos). Algunas revistas
científicas ya han actualizado sus políticas de publicación para aclarar que los modelos
de lenguaje como ChatGPT no satisfacen sus criterios de atribución de “autoría”.
   4. Supervisión. El profesor podrá usar herramientas como GPTZero u otras herramientas
digitales para detectar infracciones asociadas al uso de las herramientas de alto riesgo.
La empresa Turinitin anunció que está trabajando en agregar un módulo de detección
de escritura realizada por IA. Además, es probable que, en el futuro cercano, las propias
herramientas generativas incluirán una “marca de agua” en sus resultados que
facilitarán rastrear si un texto fue producido por IA. En todo caso, la principal estrategia
usada por el profesor para evitar los riesgos asociados a la integridad académica será
pedagógica.
   5.Acompañamiento del profesor. El profesor dispondrá tiempo de la clase para explicar
qué es la IA, qué son los modelos de lenguaje a partir de los cuales funcionan
herramientas como ChatGPT, qué oportunidades y riesgos generan para los trabajos
académicos y profesionales, y cuáles son las implicaciones éticas, ambientales y de
derechos humanos asociadas al uso de estas herramientas. El profesor siempre estará
disponible para aclarar el alcance de esta política, para discutir y co-crear la política, y
para resolver preguntas puntuales sobre el uso de IA.
4. Por qué es necesario el uso informado, ético y responsable de IA en la asignatura:
   * El uso de herramientas de IA debe ser informado, ético y responsable por – al menos –
cuatro tipos de motivos: (1) porque las herramientas de IA no siempre son confiables;
(2) porque hay riesgos de que ciertos usos afecten negativamente los procesos de
aprendizaje (3) por el riesgo de que los usuarios traten la IA como si fuera un humano;
y, (4) porque el uso de las herramientas tiene implicaciones éticas y derechos humanos
debido a la manera como fueron desarrolladas y/o porque algunas herramientas 
3
pueden replicar o amplificar problemáticas sociales tales como la discriminación. A
continuación, se explica cada de los cuatro motivos en detalle:
   * En primer lugar, porque sus respuestas NO siempre son confiables a pesar de que , por
ejemplo, una IA generadora produce textos que tienen apariencia de ser convincentes.
Los sistemas basados en Large Language Models (LLMs) como ChatGPT no funcionan
con la precisión de otras herramientas usadas en ambientes de aprendizaje como las
calculadoras. De hecho, ChatGPT tiende a incluir en sus respuestas información falsa o
fantasiosa. Estos sistemas no distinguen lo verdadero de lo falso. ¿Por qué ocurre esto?
Los LLMs hilan palabras a partir de inferencias probabilísticas de los datos con los cuales
fueron entrenados, pero no tienen la capacidad de entender lo que producen ni
asocian significados a las palabras que emiten (son “loras estocásticas”).
Recientemente un medio de comunicación que usó una herramienta tipo ChatGPT para
escribir textos tuvo que publicar correcciones a múltiples artículos debido a las graves
imprecisiones que contenían.
   * En segundo lugar, porque el uso de herramientas generadoras de texto puede
desestimular la motivación de los estudiantes para escribir y pensar por su propia
cuenta. Vale la pena reiterar que las actividades de aprendizaje de esta asignatura
buscan desarrollar sus habilidades cognitivas y que esta política busca evitar que
algunas herramientas de IA se conviertan en mecanismos de plagio automatizadas.
Además, algunas herramientas, como ChatGPT, pueden convertirse en mecanismos de
plagio automatizadas.
   * En tercer lugar, por el riesgo de que los usuarios, consciente o inconscientemente,traten la conducta de la IA como si fuera humana (Efecto Eliza). Por ejemplo, las herramientas basadas en LLMs no “entienden” los textos que producen, simplemente imitan patrones de lenguaje a partir de la síntesis de grandes volúmenes de datos a partir de los cuales generan las secuencias de palabras.
   * En cuarto lugar, por las implicaciones de éticas y derechos humanos asociadas al uso de ciertos sistemas de IA dado que: algunas herramientas tienden a reproducir o amplificar estereotipos derogatorios y discriminatorios asociados al género, raza, etnia o discapacidad; las tecnologías podrían haber sido desarrolladas a partir de la violación masiva de derechos de autor; algunas herramientas habrían sido desarrolladas en contextos de explotación laboral; el desarrollo y operación de dichos sistemas genera
una huella de carbono considerable; y, la potencial violación a los derechos de privacidad y de protección de datos personales de quienes las usan.
5. Política abierta al cambio:
   * La IA es un conjunto de herramientas que se transforma rápidamente, razón por la cual esta política permanecerá abierta a futuras evaluaciones, modificaciones y revisiones.
   * Desde el comienzo del semestre y a lo largo del semestre el profesor abriría espacios para discutir esta política con los/las estudiantes y, de ser necesario, podrán introducirse modificaciones a la política a partir de ejercicios de co-creación. 
   * Agradezco a mis colegas y demás personas que aportaron sus comentarios y críticas a versiones iniciales de esta política.
   * A quienes estén interesados en los retos que las IA generativas generan para quienes enseñamos en las universidades les recomiendo consultar este documento de las profesoras Anna Mills y Lauren M. E. Goodlad.
   * Agradezco a quien quiera enviar retroalimentación adicional o sugerencias a mi correo institucional: juandavid.gutierrez@urosario.edu.co.
